{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from lookalikes import *\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Characteristics Model Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "name = \"characteristic_googlenet_25\"\n",
    "\n",
    "scores, y_pred, y_true, y_labels = load_raw_eval_data(name)\n",
    "\n",
    "ps = {}\n",
    "ps_ed = {}\n",
    "for p in range(0, 100):\n",
    "    p = p / 100\n",
    "    ps[p] = []\n",
    "    ps_ed[p] = 0\n",
    "    for i in range(len(y_labels)):\n",
    "        t = y_labels[i]\n",
    "        tau_hat = species_from_feats(y_pred[i], p)\n",
    "        ps[p].append(performance(tau_hat, t))\n",
    "\n",
    "        real_ed = get_edibility(t)\n",
    "        pred_ed = np.argmax(np.bincount(tau_hat)) if len(tau_hat) > 0 else -1\n",
    "        ps_ed[p] += 1 if real_ed == pred_ed else 0\n",
    "\n",
    "    ps[p] = np.mean(ps[p])\n",
    "    ps_ed[p] = ps_ed[p]/len(y_labels)\n",
    "\n",
    "keys = np.array(list(ps.keys()))\n",
    "values = np.array(list(ps.values()))\n",
    "save_performance_data(keys, values, name)\n",
    "\n",
    "ed_keys = np.array(list(ps_ed.keys()))\n",
    "ed_values = np.array(list(ps_ed.values()))\n",
    "save_performance_data(ed_keys, ed_values, name+\"_ed\", edibility=True)"
   ]
  },
  {
   "source": [
    "## Direct Model Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "name = \"direct\"\n",
    "\n",
    "scores, y_pred, y_true, y_labels = load_raw_eval_data(name)\n",
    "\n",
    "ps = {}\n",
    "ps_ed = {}\n",
    "for p in range(0, 100):\n",
    "    p = p / 100\n",
    "    ps[p] = []\n",
    "    ps_ed[p] = 0\n",
    "    for i in range(len(y_labels)):\n",
    "        t = y_labels[i]\n",
    "        i_scores = list(enumerate(scores[i]))\n",
    "        sorted_scores = sorted(i_scores, key=lambda x:x[1], reverse=True)\n",
    "        size_of_tau_hat = len(lookalikes(sorted_scores[0][0], p))\n",
    "        tau_hat = [idx for (idx, val) in sorted_scores[:size_of_tau_hat]]\n",
    "        ps[p].append(performance(tau_hat, t))\n",
    "\n",
    "        real_ed = get_edibility(t)\n",
    "        pred_ed = np.argmax(np.bincount(tau_hat)) if len(tau_hat) > 0 else -1\n",
    "        ps_ed[p] += 1 if real_ed == pred_ed else 0\n",
    "\n",
    "    ps[p] = np.mean(ps[p])\n",
    "    ps_ed[p] = ps_ed[p]/len(y_labels)\n",
    "\n",
    "keys = np.array(list(ps.keys()))\n",
    "values = np.array(list(ps.values()))\n",
    "save_performance_data(keys, values, name)\n",
    "\n",
    "ed_keys = np.array(list(ps_ed.keys()))\n",
    "ed_values = np.array(list(ps_ed.values()))\n",
    "save_performance_data(ed_keys, ed_values, name+'_ed', edibility=True)"
   ]
  }
 ]
}