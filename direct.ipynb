{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361264bitcs682conda6c2838d37f0541629036f4dfbc2777db",
   "display_name": "Python 3.6.12 64-bit ('cs682': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Direct Model\n",
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ItemsViewHDF5(<HDF5 file \"bolete.h5\" (mode r)>)\nbolete-characteristics <HDF5 dataset \"bolete-characteristics\": shape (38, 1873), type \"|u1\">\nbolete-edibility <HDF5 dataset \"bolete-edibility\": shape (5, 1873), type \"|u1\">\nbolete-images <HDF5 dataset \"bolete-images\": shape (3, 512, 512, 1873), type \"|u1\">\nbolete-labels <HDF5 dataset \"bolete-labels\": shape (1873,), type \"<u8\">\n\nbolete-characteristics (38, 1873)\nbolete-edibility (5, 1873)\nbolete-images (3, 512, 512, 1873)\nbolete-labels (1873,)\n"
     ]
    }
   ],
   "source": [
    "from load_database import load_bolete_data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "data = load_bolete_data()\n",
    "print()\n",
    "for k in data.keys():\n",
    "    print(k, np.shape(data[k]))"
   ]
  },
  {
   "source": [
    "## Partition Data\n",
    "Divide data in train and held-out test partitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_tt = StratifiedShuffleSplit(n_splits=1, test_size=0.3, train_size=0.7, random_state=0)\n",
    "X, y = data[\"bolete-images\"].T, data[\"bolete-labels\"].T\n",
    "for train_idx, test_idx in sss_tt.split(X, y):\n",
    "    train_x = X[train_idx]\n",
    "    train_y_sp = data[\"bolete-labels\"].T[train_idx]\n",
    "    train_y_ed = data[\"bolete-edibility\"].T[train_idx]\n",
    "    train_y_ch = data[\"bolete-characteristics\"].T[train_idx]\n",
    "\n",
    "    test_x = X[test_idx]\n",
    "    test_y_sp = data[\"bolete-labels\"].T[test_idx]\n",
    "    test_y_ed = data[\"bolete-edibility\"].T[test_idx]\n",
    "    test_y_ch = data[\"bolete-characteristics\"].T[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of species: 178\nN = 1873, H = 512, W = 512, C = 3\n"
     ]
    }
   ],
   "source": [
    "M = np.size(np.unique(data[\"bolete-labels\"]))\n",
    "N, H, W, C = np.shape(data[\"bolete-images\"].T)\n",
    "print(\"Number of species:\", M)\n",
    "print(\"N = {}, H = {}, W = {}, C = {}\".format(N, H, W, C))"
   ]
  },
  {
   "source": [
    "## Setup PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "USE_GPU = False\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "source": [
    "## Define training and accuracy procedure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return acc\n",
    "\n",
    "def train(loader, model, optimizer, loader_train, loader_val, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    history = []\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = x.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                history.append([loss.item(), check_accuracy(loader_val, model)])\n",
    "                print()\n",
    "    return history"
   ]
  },
  {
   "source": [
    "## Define models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model():\n",
    "    # copy final model from ass igment 2\n",
    "    class Flatten(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return flatten(x)\n",
    "\n",
    "    channel_1 = 32\n",
    "    channel_2 = 24\n",
    "    channel_3 = 16\n",
    "    hidden_dim = 150\n",
    "    torch.manual_seed(0)\n",
    "    learning_rate = 3e-3 # 1e-2\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(C, channel_1, kernel_size=5, padding=2),\n",
    "        # nn.GroupNorm(4,channel_1),\n",
    "        nn.BatchNorm2d(channel_1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1),\n",
    "        # nn.GroupNorm(4,channel_2),\n",
    "        nn.BatchNorm2d(channel_2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Conv2d(channel_2, channel_3, kernel_size=3, padding=1),\n",
    "        # nn.GroupNorm(4,channel_3),\n",
    "        nn.BatchNorm2d(channel_3),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        Flatten(),\n",
    "        nn.Linear(channel_3 * H * W, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, M)\n",
    "    )\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                        momentum=0.8, nesterov=True)\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "source": [
    "## Cross Validate on training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1048, 512, 512, 3) (1048,) (263, 512, 512, 3) (263,)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'X' and 'Y'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-23067fe9dd20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_val_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_val_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'X' and 'Y'"
     ]
    }
   ],
   "source": [
    "folds = 1\n",
    "sss = StratifiedShuffleSplit(n_splits=folds, random_state=0, test_size=0.2, train_size=0.8)\n",
    "for train_index, val_index in sss.split(train_x, train_y_sp):\n",
    "    k_train_x, k_val_x = train_x[train_index], train_x[val_index]\n",
    "    k_train_y, k_val_y = train_y_sp[train_index], train_y_sp[val_index]\n",
    "    print(np.shape(k_train_x), np.shape(k_train_y), np.shape(k_val_x), np.shape(k_val_y))\n",
    "    model, optimizer = simple_model()\n",
    "\n",
    "    loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "    loader_val = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "    train(model, optimizer, loader_train, loader_val, epochs=1)"
   ]
  }
 ]
}